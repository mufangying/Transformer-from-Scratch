{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JSUM3l2wTNaK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nsANpQ8qTYet"
      },
      "outputs": [],
      "source": [
        "\n",
        "#1.位置编码\n",
        "class PositionalEncoding(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, d_model, max_len=5000):  # d_model 是词嵌入维度，max_len 是序列的最大长度\n",
        "        super().__init__()  # 调用父类 nn.Module 的构造函数\n",
        "\n",
        "        # 创建一个形状为 (max_len, d_model) 的零矩阵，用于存储位置编码\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # 生成一个形状为 (max_len, 1) 的张量，表示序列中的每个位置\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # 计算分母项，使用 e^( - (2i / d_model) * log(10000) )\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # 计算 sin 值并赋值给偶数索引（0, 2, 4, ...）的维度\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # 计算 cos 值并赋值给奇数索引（1, 3, 5, ...）的维度\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # 添加 batch 维度，使形状变为 (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # 将 pe 注册为 buffer，使其在训练过程中不会被当作模型参数更新\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将位置编码加到输入 x 上，x 的形状为 (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1)]  # 只取与输入序列长度匹配的部分\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGonHCJtTpja",
        "outputId": "5a2d15c3-4b72-4012-efda-71529fe1d259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.7195,  0.1422, -0.3647,  ...,  0.9219,  0.6199, -0.0868],\n",
            "         [-1.5566, -0.1231,  0.7637,  ..., -0.1128,  0.2161,  0.7489],\n",
            "         [-0.9458, -0.0855, -0.7524,  ..., -1.9013,  1.7498,  0.1070],\n",
            "         ...,\n",
            "         [-0.0923,  0.4496,  0.2249,  ...,  0.4902, -0.6342, -0.7123],\n",
            "         [ 0.1207,  0.9847,  1.2898,  ...,  1.3538, -0.2600,  0.6467],\n",
            "         [-1.1969,  0.6190,  0.1341,  ...,  0.6129, -0.2538,  0.2888]],\n",
            "\n",
            "        [[-0.0290,  1.3108,  0.1407,  ..., -0.2343, -1.0975,  0.2140],\n",
            "         [-0.8824,  0.5006, -1.7399,  ..., -1.3901, -0.7364, -0.4741],\n",
            "         [ 2.1290,  1.3590,  1.9984,  ..., -0.0939, -1.7093,  0.2820],\n",
            "         ...,\n",
            "         [ 0.2173,  1.0973,  0.3096,  ...,  0.4675, -1.0527, -0.3312],\n",
            "         [-1.6302,  1.8849, -0.8522,  ..., -1.9886, -0.3139, -2.6081],\n",
            "         [ 2.8070,  1.1513, -0.6044,  ..., -1.0325,  1.2977, -0.8982]]])\n",
            "tensor([[[ 0.7195,  1.1422, -0.3647,  ...,  1.9219,  0.6199,  0.9132],\n",
            "         [-0.7151,  0.4172,  1.5856,  ...,  0.8872,  0.2162,  1.7489],\n",
            "         [-0.0365, -0.5017,  0.1840,  ..., -0.9013,  1.7500,  1.1070],\n",
            "         ...,\n",
            "         [-0.0304,  1.4477,  1.0231,  ...,  1.4887, -0.5814,  0.2863],\n",
            "         [ 0.9941,  1.4718,  2.2396,  ...,  2.3523, -0.2072,  1.6453],\n",
            "         [-0.3151,  0.1473,  0.4181,  ...,  1.6114, -0.2009,  1.2874]],\n",
            "\n",
            "        [[-0.0290,  2.3108,  0.1407,  ...,  0.7657, -1.0975,  1.2140],\n",
            "         [-0.0409,  1.0409, -0.9180,  ..., -0.3901, -0.7363,  0.5259],\n",
            "         [ 3.0383,  0.9428,  2.9348,  ...,  0.9061, -1.7091,  1.2820],\n",
            "         ...,\n",
            "         [ 0.2793,  2.0953,  1.1078,  ...,  1.4661, -1.0000,  0.6674],\n",
            "         [-0.7569,  2.3720,  0.0977,  ..., -0.9901, -0.2610, -1.6095],\n",
            "         [ 3.6888,  0.6796, -0.3204,  ..., -0.0340,  1.3507,  0.1004]]])\n"
          ]
        }
      ],
      "source": [
        "positional_encoding=PositionalEncoding(512)\n",
        "x=torch.randn(2,512,512)\n",
        "print(x)\n",
        "x=positional_encoding(x)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ySi4QwlBTxHD"
      },
      "outputs": [],
      "source": [
        "#2.多头注意力大类\n",
        "class MultiHeadAttention(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        \"\"\"\n",
        "        d_model: 输入嵌入维度\n",
        "        nhead: 头数（多头注意力）\n",
        "        dropout: dropout 比例\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 断言确保 d_model 可以被 nhead 整除（每个头分配相同的维度）\n",
        "        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n",
        "\n",
        "        self.nhead = nhead  # 记录多头数量\n",
        "        self.d_k = d_model // nhead  # 每个头的维度大小\n",
        "\n",
        "        # 定义可学习的线性变换矩阵\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # 查询 (Query) 权重\n",
        "        self.W_k = nn.Linear(d_model, d_model)  # 键 (Key) 权重\n",
        "        self.W_v = nn.Linear(d_model, d_model)  # 值 (Value) 权重\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # 输出 (Output) 权重\n",
        "\n",
        "        # 定义 dropout 层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算缩放点积注意力\n",
        "        Q: 查询矩阵 (batch_size, nhead, seq_len, d_k)\n",
        "        K: 键矩阵 (batch_size, nhead, seq_len, d_k)\n",
        "        V: 值矩阵 (batch_size, nhead, seq_len, d_k)\n",
        "        mask: 位置掩码 (batch_size, 1, 1, seq_len) 或 None\n",
        "        \"\"\"\n",
        "        # 计算 QK^T / sqrt(d_k) (缩放点积)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 如果 mask 不为空，则将填充位置的得分设为一个极小值（-1e9），使其 softmax 之后趋近于 0\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 计算 softmax，得到注意力权重\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # 应用 dropout\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # 计算最终的注意力加权输出\n",
        "        return torch.matmul(attn, V)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算多头注意力\n",
        "        Q, K, V: (batch_size, seq_len, d_model)\n",
        "        mask: (batch_size, 1, 1, seq_len) 或 None\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)  # 获取 batch 大小\n",
        "\n",
        "        # 线性变换 Q、K、V，并将其拆分为多头 (batch_size, seq_len, d_model) -> (batch_size, seq_len, nhead, d_k)\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 计算注意力\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # 调整维度，使其恢复为 (batch_size, seq_len, d_model)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, -1, self.nhead * self.d_k)\n",
        "\n",
        "        # 通过最终的线性变换 W_o\n",
        "        return self.W_o(attn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rl3B_fXnUTYG"
      },
      "outputs": [],
      "source": [
        "#3.前馈神经网络\n",
        "class PositionWiseFFN(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        d_model: 输入和输出的特征维度（Transformer 的嵌入维度）\n",
        "        d_ff: 前馈网络中间层的维度（通常比 d_model 大很多，例如 2048）\n",
        "        dropout: Dropout 比例，防止过拟合\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义两层全连接网络\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层，扩展特征维度\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层，将维度变回 d_model\n",
        "\n",
        "        # Dropout 层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 激活函数，使用 ReLU\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        x: 输入张量，形状为 (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 计算第一层全连接 + 激活函数 + Dropout，然后通过第二层全连接\n",
        "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zcnGP_6kUXqH"
      },
      "outputs": [],
      "source": [
        "# 4. Transformer 编码器层（Encoder Layer）\n",
        "\n",
        "class EncoderLayer(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        d_model: 输入的嵌入维度\n",
        "        nhead: 多头注意力的头数\n",
        "        d_ff: 前馈神经网络的隐藏层维度\n",
        "        dropout: Dropout 比例，防止过拟合\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 多头自注意力层\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # 位置前馈神经网络 (FFN)\n",
        "        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer Normalization (层归一化)\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # 第一层归一化（针对自注意力层的输出）\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # 第二层归一化（针对前馈网络的输出）\n",
        "\n",
        "        # Dropout 层，防止过拟合\n",
        "        self.dropout1 = nn.Dropout(dropout)  # 用于自注意力层的输出\n",
        "        self.dropout2 = nn.Dropout(dropout)  # 用于前馈网络的输出\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        src: 输入序列张量，形状为 (batch_size, seq_len, d_model)\n",
        "        src_mask: 掩码张量，形状为 (batch_size, 1, 1, seq_len)，用于防止注意力关注填充部分\n",
        "        \"\"\"\n",
        "        # 计算多头自注意力\n",
        "        src2 = self.self_attn(src, src, src, src_mask)\n",
        "\n",
        "        # 残差连接 + Dropout + LayerNorm\n",
        "        src = src + self.dropout1(src2)  # 残差连接\n",
        "        src = self.norm1(src)  # 归一化\n",
        "\n",
        "        # 计算前馈神经网络\n",
        "        src2 = self.ffn(src)\n",
        "\n",
        "        # 残差连接 + Dropout + LayerNorm\n",
        "        src = src + self.dropout2(src2)  # 残差连接\n",
        "        src = self.norm2(src)  # 归一化\n",
        "\n",
        "        return src  # 返回当前编码器层的输出\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "bmIC3ersgk-Y"
      },
      "outputs": [],
      "source": [
        "# 5. Transformer 解码器层（Decoder Layer）\n",
        "# 导入必要的库\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        d_model: 输入的嵌入维度\n",
        "        nhead: 多头注意力的头数\n",
        "        d_ff: 前馈神经网络的隐藏层维度\n",
        "        dropout: Dropout 比例，防止过拟合\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 多头自注意力层（解码器中的自注意力）\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # 多头交叉注意力层（解码器与编码器之间的交叉注意力）\n",
        "        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # 位置前馈神经网络 (FFN)\n",
        "        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer Normalization (层归一化)\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # 第一层归一化（针对自注意力层的输出）\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # 第二层归一化（针对交叉注意力层的输出）\n",
        "        self.norm3 = nn.LayerNorm(d_model)  # 第三层归一化（针对前馈网络的输出）\n",
        "\n",
        "        # Dropout 层，防止过拟合\n",
        "        self.dropout1 = nn.Dropout(dropout)  # 用于自注意力层的输出\n",
        "        self.dropout2 = nn.Dropout(dropout)  # 用于交叉注意力层的输出\n",
        "        self.dropout3 = nn.Dropout(dropout)  # 用于前馈网络的输出\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        tgt: 目标序列张量，形状为 (batch_size, tgt_seq_len, d_model)\n",
        "        memory: 编码器输出的记忆张量，形状为 (batch_size, src_seq_len, d_model)\n",
        "        tgt_mask: 目标序列的掩码张量，形状为 (batch_size, 1, 1, tgt_seq_len) 或 None\n",
        "        memory_mask: 编码器输出的掩码张量，形状为 (batch_size, 1, 1, src_seq_len) 或 None\n",
        "        \"\"\"\n",
        "        # 自注意力（目标序列内部的注意力）\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "\n",
        "        # 残差连接 + Dropout + LayerNorm\n",
        "        tgt = tgt + self.dropout1(tgt2)  # 残差连接\n",
        "        tgt = self.norm1(tgt)  # 归一化\n",
        "\n",
        "        # 交叉注意力（目标序列和编码器输出之间的注意力）\n",
        "        tgt2 = self.cross_attn(tgt, memory, memory, memory_mask)\n",
        "\n",
        "        # 残差连接 + Dropout + LayerNorm\n",
        "        tgt = tgt + self.dropout2(tgt2)  # 残差连接\n",
        "        tgt = self.norm2(tgt)  # 归一化\n",
        "\n",
        "        # 位置前馈网络 (FFN)\n",
        "        tgt2 = self.ffn(tgt)\n",
        "\n",
        "        # 残差连接 + Dropout + LayerNorm\n",
        "        tgt = tgt + self.dropout3(tgt2)  # 残差连接\n",
        "        tgt = self.norm3(tgt)  # 归一化\n",
        "\n",
        "        return tgt  # 返回当前解码器层的输出\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3-dh6R-9gmza"
      },
      "outputs": [],
      "source": [
        "# 6. Transformer 编码器（Encoder）\n",
        "\n",
        "class Encoder(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8,\n",
        "                 num_layers=6, d_ff=2048, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        vocab_size: 词汇表的大小\n",
        "        d_model: 嵌入维度（输入特征的维度）\n",
        "        nhead: 多头注意力的头数\n",
        "        num_layers: 编码器层数\n",
        "        d_ff: 前馈网络的隐藏层维度\n",
        "        dropout: Dropout 比例，防止过拟合\n",
        "        max_len: 最大序列长度，用于位置编码\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 嵌入层，将词索引转换为词向量\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # 位置编码层，增加位置位置信息\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # 编码器层，构造多个 EncoderLayer 层\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, nhead, d_ff, dropout)  # 每个 EncoderLayer 都是之前定义的\n",
        "            for _ in range(num_layers)  # 构造 num_layers 个编码器层\n",
        "        ])\n",
        "\n",
        "        # Dropout 层，防止过拟合\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        src: 输入序列张量，形状为 (batch_size, seq_len)\n",
        "        src_mask: 输入序列的掩码张量，形状为 (batch_size, 1, 1, seq_len) 或 None\n",
        "        \"\"\"\n",
        "        # 将输入的单词索引通过嵌入层转换为词向量，并进行缩放\n",
        "        src = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)  # 缩放嵌入向量\n",
        "\n",
        "        # 添加位置编码信息\n",
        "        src = self.pos_encoding(src)  # 将词嵌入与位置编码相加\n",
        "\n",
        "        # 应用 Dropout，防止过拟合\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        # 通过多个 EncoderLayer 进行处理\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)  # 通过每个 EncoderLayer\n",
        "\n",
        "        return src  # 返回经过多个编码器层后的输出\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "zsS54kiVgpLX"
      },
      "outputs": [],
      "source": [
        "# 7. Transformer 解码器（Decoder）\n",
        "class Decoder(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8,\n",
        "                 num_layers=6, d_ff=2048, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        vocab_size: 词汇表的大小\n",
        "        d_model: 嵌入维度（输入特征的维度）\n",
        "        nhead: 多头注意力的头数\n",
        "        num_layers: 解码器层数\n",
        "        d_ff: 前馈网络的隐藏层维度\n",
        "        dropout: Dropout 比例，防止过拟合\n",
        "        max_len: 最大序列长度，用于位置编码\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 目标序列嵌入层，将词索引转换为词向量\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # 位置编码层，增加位置位置信息\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # 解码器层，构造多个 DecoderLayer 层\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, nhead, d_ff, dropout)  # 每个 DecoderLayer 都是之前定义的\n",
        "            for _ in range(num_layers)  # 构造 num_layers 个解码器层\n",
        "        ])\n",
        "\n",
        "        # Dropout 层，防止过拟合\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 输出投影层，将解码器的输出映射回词汇表大小\n",
        "        self.projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        tgt: 目标序列张量，形状为 (batch_size, tgt_seq_len)\n",
        "        memory: 编码器输出的记忆张量，形状为 (batch_size, src_seq_len, d_model)\n",
        "        tgt_mask: 目标序列的掩码张量，形状为 (batch_size, 1, 1, tgt_seq_len) 或 None\n",
        "        memory_mask: 编码器输出的掩码张量，形状为 (batch_size, 1, 1, src_seq_len) 或 None\n",
        "        \"\"\"\n",
        "        # 将输入的目标序列单词索引转换为词向量，并进行缩放\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)  # 缩放嵌入向量\n",
        "\n",
        "        # 添加位置编码信息\n",
        "        tgt = self.pos_encoding(tgt)  # 将词嵌入与位置编码相加\n",
        "\n",
        "        # 应用 Dropout，防止过拟合\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        # 通过多个 DecoderLayer 进行处理\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask, memory_mask)  # 通过每个 DecoderLayer\n",
        "\n",
        "        # 通过投影层将解码器的输出映射回词汇表大小\n",
        "        return self.projection(tgt)  # 形状为 (batch_size, tgt_seq_len, vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "_W_nyeTggrIV"
      },
      "outputs": [],
      "source": [
        "# 8. Transformer 模型\n",
        "class Transformer(nn.Module):  # 继承 PyTorch 的 nn.Module\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size,\n",
        "                 d_model=512, nhead=8, num_layers=6,\n",
        "                 d_ff=2048, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        src_vocab_size: 源语言的词汇表大小\n",
        "        tgt_vocab_size: 目标语言的词汇表大小\n",
        "        d_model: 嵌入维度（输入特征的维度）\n",
        "        nhead: 多头注意力的头数\n",
        "        num_layers: 编码器和解码器的层数\n",
        "        d_ff: 前馈神经网络的隐藏层维度\n",
        "        dropout: Dropout 比例，防止过拟合\n",
        "        max_len: 最大序列长度，用于位置编码\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义 Transformer 的编码器\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, nhead,\n",
        "                              num_layers, d_ff, dropout, max_len)\n",
        "\n",
        "        # 定义 Transformer 的解码器\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, nhead,\n",
        "                              num_layers, d_ff, dropout, max_len)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        src: 源序列张量，形状为 (batch_size, src_seq_len)\n",
        "        tgt: 目标序列张量，形状为 (batch_size, tgt_seq_len)\n",
        "        src_mask: 源序列的掩码张量，形状为 (batch_size, 1, 1, src_seq_len) 或 None\n",
        "        tgt_mask: 目标序列的掩码张量，形状为 (batch_size, 1, 1, tgt_seq_len) 或 None\n",
        "        \"\"\"\n",
        "        # 通过编码器处理源序列\n",
        "        memory = self.encoder(src, src_mask)  # 形状: (batch_size, src_seq_len, d_model)\n",
        "\n",
        "        # 通过解码器处理目标序列，结合编码器的输出\n",
        "        output = self.decoder(tgt, memory, tgt_mask)  # 形状: (batch_size, tgt_seq_len, vocab_size)\n",
        "\n",
        "        return output  # 返回解码器的最终输出\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "b9jF49Pzgs5P"
      },
      "outputs": [],
      "source": [
        "# 9. mask生成函数\n",
        "\n",
        "def generate_mask(src, tgt, pad_idx=0):\n",
        "    \"\"\"\n",
        "    生成 Transformer 所需的掩码（mask）。\n",
        "\n",
        "    参数：\n",
        "    src: 源序列张量，形状为 (batch_size, src_seq_len)\n",
        "    tgt: 目标序列张量，形状为 (batch_size, tgt_seq_len)\n",
        "    pad_idx: 填充索引（默认为 0），用于标记填充部分\n",
        "\n",
        "    返回：\n",
        "    src_mask: 源序列的掩码，形状为 (batch_size, 1, 1, src_seq_len)\n",
        "    tgt_mask: 目标序列的掩码，形状为 (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "    \"\"\"\n",
        "\n",
        "    # 生成源序列的 Padding Mask：用于防止模型关注填充 (PAD) 位置\n",
        "    # (src != pad_idx) 生成一个布尔张量，标记非 PAD 位置为 True，PAD 位置为 False\n",
        "    # 通过 unsqueeze(1) 和 unsqueeze(2) 扩展维度，使其形状变为 (batch_size, 1, 1, src_seq_len)\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)  # 形状: (B, 1, 1, S)\n",
        "\n",
        "    # 生成目标序列的 Padding Mask\n",
        "    # 形状: (batch_size, 1, tgt_seq_len, 1)\n",
        "    tgt_pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "\n",
        "    # 计算目标序列的 Look-Ahead Mask (防止解码器看到未来的信息)\n",
        "    seq_len = tgt.size(1)  # 目标序列长度\n",
        "    look_ahead_mask = torch.ones(seq_len, seq_len).tril().bool()  # 生成一个下三角矩阵\n",
        "\n",
        "    # 由于 Look-Ahead Mask 应该应用到整个 batch 上，因此需要扩展维度\n",
        "    # (1, 1, tgt_seq_len, tgt_seq_len) -> 适用于 batch 维度\n",
        "    look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # 目标序列的总掩码 = Padding Mask & Look-Ahead Mask\n",
        "    # 只有当 tgt_pad_mask 和 look_ahead_mask 都为 True 时，模型才能关注该位置\n",
        "    tgt_mask = tgt_pad_mask & look_ahead_mask.to(tgt.device)  # 形状: (B, 1, T, T)\n",
        "\n",
        "    return src_mask, tgt_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KywRzN8hDd_",
        "outputId": "377592ec-4b1c-46a9-c7da-abda54296f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[ True, False, False, False],\n",
            "          [ True,  True, False, False],\n",
            "          [ True,  True,  True, False],\n",
            "          [ True,  True,  True,  True]]],\n",
            "\n",
            "\n",
            "        [[[ True, False, False, False],\n",
            "          [ True,  True, False, False],\n",
            "          [ True,  True,  True, False],\n",
            "          [False, False, False, False]]]])\n",
            "输出形状: tensor([ 0.7169, -0.1168,  1.2205,  ..., -0.0685,  0.1231,  0.1344],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 定义源语言和目标语言的词汇表大小\n",
        "src_vocab = 10000  # 源语言词汇表大小\n",
        "tgt_vocab = 8000   # 目标语言词汇表大小\n",
        "\n",
        "# 创建 Transformer 模型，传入词汇表大小和其他超参数\n",
        "model = Transformer(src_vocab, tgt_vocab)\n",
        "\n",
        "# 模拟输入，假设每个序列的长度是 4，batch_size 是 2\n",
        "src = torch.LongTensor([[1, 2, 3, 4], [5, 6, 0, 0]])  # 0是padding，形状为 (batch_size, seq_len)\n",
        "tgt = torch.LongTensor([[10, 20, 30, 40], [10, 60, 70, 0]])  # 同样，0是padding，形状为 (batch_size, seq_len)\n",
        "\n",
        "# 生成源序列和目标序列的掩码（mask），用于在计算注意力时防止模型关注填充位置\n",
        "src_mask, tgt_mask = generate_mask(src, tgt)\n",
        "print(tgt_mask)\n",
        "\n",
        "\n",
        "output = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "# 打印输出的形状\n",
        "print(\"输出形状:\", output.shape)  # 预期输出形状是 (batch_size, tgt_seq_len-1, vocab_size)，即 (2, 3, 8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfwBmGIVoJ4q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
